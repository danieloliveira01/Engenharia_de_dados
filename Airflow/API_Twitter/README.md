Engenharia de Dados com Apache Airflow e Consumo de API

Este projeto tem como objetivo desenvolver uma pipeline de engenharia de dados utilizando o Apache Airflow para realizar a extra√ß√£o de dados de uma API contendo postagens do Twitter, armazenando-os em um Data Lake para posterior an√°lise por equipes de Ci√™ncia de Dados e Machine Learning.

üîÑ Arquitetura da Pipeline

A pipeline segue as etapas cl√°ssicas de ETL:

Extra√ß√£o: Realiza requisi√ß√µes para a API do Twitter e coleta as postagens.

Transforma√ß√£o: Limpeza e padroniza√ß√£o dos dados coletados.

Carregamento: Armazenamento no Data Lake, organizando os dados em diret√≥rios particionados por data de execu√ß√£o, facilitando consultas futuras.

üõ†Ô∏è Tecnologias Utilizadas

Python ‚Üí Desenvolvimento dos scripts de extra√ß√£o, transforma√ß√£o e carregamento.

Apache Airflow ‚Üí Orquestra√ß√£o e automa√ß√£o da pipeline.

API do Twitter ‚Üí Fonte de dados.

‚úÖ Benef√≠cios da Solu√ß√£o

Automa√ß√£o completa do fluxo de dados

Organiza√ß√£o dos arquivos por data de execu√ß√£o

Melhor governan√ßa e rastreabilidade dos dados

Base preparada para consumo eficiente por modelos de dados e an√°lises

‚ñ∂Ô∏è Como Executar

Certifique-se de que o Apache Airflow est√° instalado e configurado corretamente.

Mova os arquivos do projeto para o diret√≥rio dags/ do seu Airflow.


